[{"content":"","date":"6 de Janeiro de 2022","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"6 de Janeiro de 2022","permalink":"/categories/code/","section":"Categories","summary":"","title":"code"},{"content":"","date":"6 de Janeiro de 2022","permalink":"/","section":"Enigma","summary":"","title":"Enigma"},{"content":"","date":"6 de Janeiro de 2022","permalink":"/categories/news/","section":"Categories","summary":"","title":"news"},{"content":"","date":"6 de Janeiro de 2022","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"6 de Janeiro de 2022","permalink":"/categories/r/","section":"Categories","summary":"","title":"R"},{"content":"    Uma boa forma de desenvolver flu√™ncia em uma linguagem de programa√ß√£o √© se submeter a resolver problemas utilizando a linguagem que voc√™ deseja aprender. Para quem tem interesse em trabalhar com computa√ß√£o cient√≠fica, um exerc√≠cio muito interessante √© selecionar alguns problemas b√°sicos na sua √°rea de atua√ß√£o e tentar resolver! Tente entender bem o problema a ser resolvido e se submeta ao exerc√≠cio de implementar uma fun√ß√£o da forma mais gen√©rica poss√≠vel.\nEsse t√≥pico sugiro meio que por acaso. Na verdade, eu estava na sala de aula, explicando aos meus alunos sobre funcionais em R e dizia a eles que nem sempre precisamos utilizar instru√ß√µes de repeti√ß√£o como for, while e repeat para repetir trechos de c√≥digos, e na aula usei o exemplo das s√©ries de Taylor. Tanto na aula, quanto aqui, deixo claro que s√©ries de Taylor n√£o √© o tema principal, muito embora o t√≠tulo da postagem possa talvez te confundir a respeito do prop√≥sito dessa postagem.\nO maior objetivo √© fazer com que voc√™ possa estudar o c√≥digo e entender os recursos utilizados para se chegar a solu√ß√£o. Procure entender o papel do funcional sapply(), da fun√ß√£o do.call(), do operador dot-dot-dot ... e do uso de fun√ß√£o an√¥nima no funcional sapply(). Esses s√£o basicamente os principais pontos do c√≥digo.\nTodavia, √© claro que eu terei que definir brevemente o que √© uma s√©rie de Taylor. Afinal, precisamos compreender, de forma clara, o que o c√≥digo tenta implementar. Tudo bem?! ü§ù\nCompreender de forma cristalina o problema √© a chave üîê para desenvolver, de forma consistente, um c√≥digo coerente e reaproveit√°vel. Tente evitar o c√≥digo espaguete, em que voc√™ at√© resolve o problema, por√©m, o reaproveitamento de c√≥digo e a manuten√ß√£o do mesmo se torna muito desafiador! Fuja desses tipos de implementa√ß√µes e projetos, pois estes n√£o acrescentam muito no seu aprendizado de programa√ß√£o.\n Fuja de c√≥digos espaguetes üçù\nAtualmente, o meu maior pavor üò± em ajudar algu√©m em programa√ß√£o √© olhar para o c√≥digo de uma pessoa e ver nele um espaguete com instru√ß√µes confusas e escritas de forma \u0026ldquo;enlinhada\u0026rdquo; (como um espaguete üçù) e cheios de gambiarras (p√©ssimas pr√°ticas de programa√ß√£o) üò•.\nEvite muitas condi√ß√µes aninhadas para n√£o cair em uma pir√¢mide de destrui√ß√£o e acabe construindo c√≥digo espaguete. Abuse, na medida do poss√≠vel, do uso de funcionais em R.\nEu ministrei uma palestra no Workshop Paraibano de Estat√≠stica, em 2020, intitulada Uso de funcionais em R: evitando loops e seus benef√≠cios e alerto um pouco sobre a necessidade de construirmos c√≥digos que possam ser reutiliz√°veis e mais f√°ceis de manter.\nCaso deseje ver:\n  Para acessar o v√≠deo da apresenta√ß√£o, clique aqui üìΩÔ∏è;\n  Para ter acesso aos slides utilizados, clique aqui üìñ.\n  Se aproximar do uso de funcionais, quando poss√≠vel, √© um √≥timo come√ßo para evitar c√≥digo espaguete. üéâ\n     ¬†    Uma √≥tima postagem que encontrei na internet e que poder√° ser √∫til para voc√™ entender sobre a import√¢ncia de se afastar de c√≥digo espaguete, em R, poder√° ser acessada clicando AQUI. E n√£o se engane, apesar do termo engra√ßado (c√≥digo espaguete), h√° diversos livros na computa√ß√£o sobre o tema.\nS√©rie de Taylor  #   Em matem√°tica, uma s√©rie de Taylor √© uma soma de fun√ß√µes, de tal forma que\n$$f(x) = \\sum_{n = 0}^{\\infty} a_n(x-a)^n,$$ com\n$$a_n = \\frac{f^{(n)}(a)}{n!},$$ em que $f^{(n)}(a)$ √© a $n$-√©sima derivada da fun√ß√£o $f$ (supostamente sendo infinitamente diferenci√°vel) avaliada no ponto $a$. Para o caso de $a = 0$, essa aproxima√ß√£o √© denominada de s√©rie de Maclaurin.\nPor exemplo, considerado $a = 0$, poderemos aproximar $f(x) = e^x$, em torno de $a = 0$ por meio da express√£o abaixo:\n$$e^x \\approx \\sum_{n = 0}^N \\frac{x^n}{n!},$$ com $N \u0026lt; \\infty$.\nImplementa√ß√£o  #   N√£o irei discutir, nos por menores, a implementa√ß√£o do c√≥digo, pois certamente seria enfadonho. Estudar o c√≥digo \u0026ldquo;n√£o espaguete\u0026rdquo; üçù de outra pessoa √© uma boa forma de adquirir conhecimento. O GitHub est√° a√≠ para voc√™ estudar um universo de c√≥digos e melhorar suas pr√°ticas de programa√ß√£o, seja em R ou qualquer outra linguagem de programa√ß√£o.\nO que eu basicamente fiz foi implementar duas fun√ß√µes:\n  A fun√ß√£o taylor(n = 1L, func, x, a = 0,...) que recebe o n√∫mero de somas n, a fun√ß√£o func a ser aproximada, um valor x e o valor de a. Tamb√©m √© poss√≠vel passar argumentos adicionais usando o operador varargs dot-dot-dot (...), em que voc√™ poder√° controlar argumentos da fun√ß√£o Deriv() do pacote Deriv utilizado para o c√°lculo das derivadas simb√≥licas de $f(x)$. O pacote encontra-se no Comprehensive R Archive Network - CRAN e poder√° ser instalado fazendo install.packages(\u0026quot;Deriv\u0026quot;)(Clausen and Sokol 2020);\n  A fun√ß√£o plot_taylor(n, func, lower, upper, a) que permitir√° que possamos visualizar graficamente a aproxima√ß√£o. Temos que n √© a quantidade de somas utilizadas na aproxima√ß√£o, func √© a fun√ß√£o que desejamos aproximar, lower √© o limite inferior do eixo x, upper √© o limite superior do eixo $x$ e a √© o ponto onde tentaremos uma boa aproxima√ß√£o em seu entorno.\n  Note que, em R, fun√ß√µes s√£o objetos de primeira classe. Isso quer dizer que, podemos criar uma fun√ß√£o dentro de outra fun√ß√£o e, sobretudo, poderemos ter uma fun√ß√£o retornando uma fun√ß√£o, como √© o caso da fun√ß√£o an() definida em taylor(). Note que a fun√ß√£o an() retorna a fun√ß√£o que implementa a derivada de func.\n Code # S√©rie de Taylor --------------------------------------------------------- taylor \u0026lt;- function(n = 1L, func, x, a = 0,...){ an \u0026lt;- function(n, ...){ Deriv::Deriv(f = func, nderiv = n, ...) } if(n == 1L) return(func(a)) sapply( X = 1L:n, FUN = \\(n, x) do.call(an(n, ...), list(x = x))/factorial(n) * (x - a)^n, x = x ) |\u0026gt; sum() + func(a) } # Testando a fun√ß√£o taylor(). taylor(n = 2L, func = \\(x) x^2, x = 0.5, a = 0)  [1] 0.75   Derivadas simb√≥licas\nConhecer derivadas de fun√ß√µes √© algo muito corriqueiro na computa√ß√£o cient√≠fica. Com o pacote Deriv voc√™ n√£o precisa se preocupar em derivar simbolicamente (analiticamente) as $n$-√©simas derivadas de uma fun√ß√£o.\n Code library(Deriv) # Derivando simbolicamente log(1-sin(x)). Perceba que # o retorno √© uma fun√ß√£o R em termos de x. Deriv(f = \\(x) log(1-sin(x)))  function (x) -(cos(x)/(1 - sin(x)))  √â importante destacar que a obten√ß√£o de derivadas simb√≥licas s√£o mais custosas que os m√©todos num√©ricos de deriva√ß√£o, sobretudo quando temos interesse de obter ordens elevadas de deriva√ß√£o. Mesmo assim, irei utilizar aqui a biblioteca Deriv, que poder√° ser bastante √∫til em diversas situa√ß√µes. Para a obten√ß√£o de derivadas num√©ricas, estude a biblioteca numDeriv (Gilbert and Varadhan 2019).\n Visualizar graficamente os resultados de uma fun√ß√£o poder√° nos ajudar na valida√ß√£o da implementa√ß√£o. Comportamentos inadequados na fun√ß√£o taylor() seriam facilmente percebidos ao utilizar a fun√ß√£o plot_taylor().\n Code library(ggplot2) library(glue) library(patchwork) plot_taylor \u0026lt;- function(n = 1L, func, a, lower = -1, upper = 1, ...){ x \u0026lt;- seq(lower, upper, length.out = 100L) y \u0026lt;- func(x) y_taylor \u0026lt;- sapply( X = x, FUN = \\(x) taylor(n = n, func = func, x = x, a = a, ...) ) data.frame( x = c(x, x), y = c(y, y_taylor), classe = c(rep(\u0026#34;f(x)\u0026#34;, 100L), rep(\u0026#34;Taylor\u0026#34;, 100L)) ) |\u0026gt; ggplot() + geom_line(aes(x = x, y = y, color = classe), size = 0.9) + geom_point(x = a, y = func(a), color = \u0026#34;blue\u0026#34;, size = 2) + ggtitle( label = \u0026#34;S√©rie de Taylor\u0026#34;, subtitle = glue(\u0026#34;Aproxima√ß√£o de uma fun√ß√£o f(x)\\n n = {n}\u0026#34;) ) + ylab(\u0026#34;f(x)\u0026#34;) + scale_color_manual(values = c(\u0026#34;black\u0026#34;, \u0026#34;tomato\u0026#34;)) + theme( text = element_text(face = \u0026#34;bold\u0026#34;) ) + labs(color = \u0026#34;\u0026#34;) } # Aproximando func em torno de \u0026#34;a\u0026#34; ---------------------------------------- p1 \u0026lt;- plot_taylor( n = 3, func = \\(x) exp(x), lower = -1, upper = 1, a = 0.5 ) p2 \u0026lt;- plot_taylor( n = 3, func = \\(x) x^3, lower = -1, upper = 1, a = 0.5 ) p3 \u0026lt;- plot_taylor( n = 3, func = \\(x) 3*x^3 + 4*x^2, lower = -1, upper = 1, a = 0 ) p4 \u0026lt;- plot_taylor( n = 3, func = \\(x) 15*x^2 - 14*x + 7, lower = -0.8, upper = 2, a = 1.5 ) (p1 + p2)/(p3 + p4) + plot_annotation(tag_levels = \u0026#34;I\u0026#34;)  Refer√™ncias  #   Clausen, Andrew, and Serguei Sokol. 2020. \u0026ldquo;Deriv: R-Based Symbolic Differentiation.\u0026rdquo; https://CRAN.R-project.org/package=Deriv.\nGilbert, Paul, and Ravi Varadhan. 2019. \u0026ldquo;numDeriv: Accurate Numerical Derivatives.\u0026rdquo; https://CRAN.R-project.org/package=numDeriv.\n","date":"6 de Janeiro de 2022","permalink":"/posts/serie_de_taylor/","section":"Posts","summary":"Uma boa forma de desenvolver flu√™ncia em uma linguagem de programa√ß√£o √© se submeter a resolver problemas utilizando a linguagem que voc√™ deseja aprender.","title":"S√©rie de Taylor, c√≥digo espaguete e outras coisas"},{"content":"    Uma breve introdu√ß√£o  #   Ci√™ncia de dados √©, sem d√∫vidas, uma √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.\n Note\nEu costumo dizer aos meus alunos que um √≥timo cientista de dados √© o profissional que sabe mais estat√≠stica que um cientista da computa√ß√£o mediano e mais computa√ß√£o de que um estat√≠stico mediano.\n Tamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine! Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.\n Dicas de linguagens\nSe voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando! Isso ir√° facilitar sua vida, pois voc√™ ter√° produtividade, aproveitando da infinidade de bibliotecas e frameworks dispon√≠veis para nossa √°rea.\nPor√©m, se mesmo assim voc√™ quiser insistir na pergunta üòü, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:\n R, veja R Core Team (2022) Python, veja Rossum (2010) Julia, veja Bezanson et al. (2017)   Se voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados (bibliotecas e frameworks), i.e., voc√™ ter√° v√°rios presentinhos, gr√°tis, para poder utilizar nos seus projetos! üéÅ\nCada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal, sendo comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o, ok? üëç\nNo meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, (veja p.¬†74 de (Bezanson et al. 2017), quando surgiu a linguagem, em 2018 utilizei para construir c√≥digos de um artigo Marinho et al. (2018), mas somente a partir da pandemia de COVID-19 foi que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando mais recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas, estruturas de dados, fun√ß√µes varargs, closures, bibliotecas como NumPy e SciPy, pedras angulares para se fazer ci√™ncia de dados em Python, entre outros assuntos. Se voc√™ veio de R ou Julia, como foi o meu caso, e quer dominar a linguagem Python, estude a documenta√ß√£o oficial da linguagem que √© √≥tima. Procure conhecer:\n  As estruturas de dados de Python: arrays, tuplas, listas, dicion√°rios e conjuntos. Nesse t√≥pico, entenda que tuplas s√£o objetos imut√°veis, listas s√£o mut√°veis e dicion√°rios s√£o objetos mut√°veis, assim como as listas, por√©m possuem palavras-chave;\n  Estude o conceito de fun√ß√µes varargs, t√©cnica muito poderosa, presente em todas √†s tr√™s linguagens, que permite generaliza√ß√µes que conduzem a c√≥digos muito mais √∫teis;\n  Estude o conceito de closures, tamb√©m presente em todas √†s tr√™s linguagens que estamos conversando, e em diversas outras. Esse conceito permite que suas fun√ß√µes possam construir novas fun√ß√µes;\n  Sobretudo, experimente a linguagem!\n   As linguagens podem conversar entre si üéâ\nVoc√™ n√£o precisar√° apenas utilizar uma mesma linguagem de programa√ß√£o em um projeto de ci√™ncia de dados que esteja trabalhando. Se existe como fazer algo na linguagem que voc√™ est√° utilizando, fa√ßa nessa linguagem! Isso ir√° diminuir o uso de depend√™ncias. Por√©m, caso algo n√£o exista para sua linguagem e voc√™ n√£o est√° afim de implementar algum m√©todo do zero, ent√£o lembre-se que voc√™ poder√° importar c√≥digo de outras linguagens.\nSe voc√™ est√°\n  em R: voc√™ poder√° importar, utilizando a biblioteca reticulate, veja Ushey, Allaire, and Tang (2022), m√©todos e objetos com estruturas de dados em Python onde ser√£o convertidos para estruturas equivalentes em R. Caso queira importar c√≥digos Julia, voc√™ poder√° utilizar a biblioteca JuliaCall (Li 2019);\n  em Julia: voc√™ poder√° importar c√≥digos R usando a biblioteca RCall. Para importar c√≥digos Python, voc√™ dever√° utilizar a biblioteca PyCall;\n  em Python: voc√™ poder√° chamar c√≥digos R utilizando a biblioteca rpy2. J√° para chamar c√≥digos Julia, veja a biblioteca PyJulia.\n   Se voc√™ j√° √© fluente em ao menos uma linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar outra(s) linguagem(ens) ser√° muito mais r√°pido. S√£o poucas semanas de estudo necess√°rias para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° üß† com a sintaxe das linguagens que voc√™ j√° programa. Normal!\nUm dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos Estimadores de M√°xima Verossimilhan√ßa - EMV. Quando estamos utilizando frameworks para ci√™ncia de dados, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.\nNa √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por \u0026ldquo;baixo dos panos\u0026rdquo; n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que precisar√° ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para obten√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward), aplicadas a problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo que considerar√° os pesos sin√°pticos da arquitetura da rede com $n$ conex√µes, isto √©, existir√° uma fun√ß√£o $f(y, w_1, w_2, \\cdots)$, em fun√ß√£o dos pesos sin√°pticos $w_i, i = 1, \u0026hellip;, n$ e da sa√≠da esperada. Como $y$ √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontrar um conjunto √≥timo de valores $w_i$ que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo backpropagation utiliza m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global de uma fun√ß√£o objetivo.\nSe voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de estat√≠stica computacional, na Se√ß√£o de Otimiza√ß√£o N√£o-linear.\n Em algum momento voc√™ tem que saber otimizar fun√ß√µes\nEm fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Escolher entre minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema em quest√£o.\nVoc√™, como um cientista de dados que √©, ou que almeja ser, ser√° o respons√°vel em descobrir se o que precisar√° fazer ser√° maximizar ou minimizar uma fun√ß√£o. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. üß†\nFun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üëç\n Estimadores de m√°xima verossimilhan√ßa - EMV  #   Para n√£o ficarmos apenas olhando c√≥digos de programa√ß√£o em tr√™s linguagens distintas (R, Julia e Python), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da fun√ß√£o de verossimilhan√ßa. Serei muito breve! üéâ\n A maior barreira üß± de uma implementa√ß√£o consistente!\nA grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o, √© n√£o saber ao certo o que desejamos implementar.\n√â por isso que irei contextualizar, de forma breve, um problema comum na estat√≠stica e ci√™ncia de dados; o problema de otimizar uma fun√ß√£o objetivo. Mais precisamente, desejaremos obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.\n Para simplificar a teoria, irei considerar algumas premissas:\n  Voc√™ tem algum conhecimento de probabilidade;\n  Considerarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por $X$;\n  A vari√°vel aleat√≥ria - v.a. $X$ √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma fun√ß√£o densidade de probabilidade - fdp $f$, tal que $f_X(x) \\geq 0$ e $\\int_{-\\infty}^{+\\infty}f_X(x), \\mathrm{d}x = 1$.\n  Na pr√°tica, o problema consiste em, por meio de um conjunto de dados, fixarmos uma fdp $f_X$. Da√≠, desejaremos encontrar os par√¢metros de fdp que faz com que $f_X$ melhor se ajuste aos dados. Os par√¢metros $\\alpha$ e $\\beta$ que far√° com que $f_X$ melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa $\\mathcal{L}$ de $f_X$, em rela√ß√£o a $\\alpha$ e $\\beta$, definida por:\n$$ \\mathcal{L}(x, \\alpha,\\beta) = \\prod_{i = 1}^n f_{X_i}(x, \\alpha, \\beta). \\qquad(1)$$ Para simplificar as coisas, como $\\log(\\cdot)$ √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de $\\alpha$ e $\\beta$ que maximizam $\\mathcal{L}$ ser√£o os mesmos que maximizam $\\log(\\mathcal{L})$, ou seja, poderemos nos concentrar em maximizar:\n$$\\ell(x, \\alpha, \\beta) = \\sum_{i = i}^n \\log[f_{X_i}(x, \\alpha, \\beta)]. \\qquad(2)$$\nSuponha que $X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)$, ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. $X$ que tem observa√ß√µes que seguem a distribui√ß√£o $Weibull(\\alpha = 2.5, \\beta = 1.5)$. Essa ser√° nossa distribui√ß√£o verdadeira!\nNa pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥\nVamos supor que voc√™, assertivamente, escolhe a fam√≠lia Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma). Existem testes de ader√™ncias para checar o quanto uma distribui√ß√£o se ajusta a um conjunto de dados. N√£o entraremos nesse assunto aqui!\n Quer ver o c√≥digo do gr√°fico? Clique aqui! library(ggplot2) # Quantidade de elementos n \u0026lt;- 550L # Par√¢metro de forma alpha \u0026lt;- 2.5 # Par√¢metro de escala beta \u0026lt;- 1.5 # Fixando uma semente, de forma a sempre obtermos a mesma amostra set.seed(0) dados \u0026lt;- data.frame( x = seq(0, 2, length.out = n), y_rand = rweibull(n, shape = alpha, scale = beta) ) dados |\u0026gt; ggplot() + geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) + ggtitle( label = \u0026#34;Histograma do conjunto de dados\u0026#34;, subtitle = \u0026#34;Na pr√°tica voc√™ s√≥ tem eles\u0026#34; ) + labs( y = \u0026#34;Densidade\u0026#34;, x = \u0026#34;x\u0026#34; ) + scale_x_continuous( limits = c(0, 2.7), n.breaks = 15 ) + scale_y_continuous( limits = c(0, 1.05), n.breaks = 15 ) + geom_function( fun = dweibull, args = list(shape = alpha, scale = 1), size = 0.8 ) + geom_function( fun = dweibull, args = list(shape = alpha, scale = 1.1), color = \u0026#34;blue\u0026#34;, size = 0.8 ) + geom_function( fun = dweibull, args = list(shape = alpha, scale = 1.2), color = \u0026#34;tomato\u0026#34;, size = 0.8 ) + geom_function( fun = dweibull, args = list(shape = alpha, scale = 1.3), color = \u0026#34;red\u0026#34;, size = 0.8 ) + geom_function( fun = dweibull, args = list(shape = alpha, scale = beta), color = \u0026#34;gold\u0026#34;, size = 0.8 )  Note que todas as fun√ß√µes densidades de probabilidades - fdps plotadas no histograma apresentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de $\\alpha$ e $\\beta$, respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequada. Precisamos escolher (estimar) adequadamente os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.\nLembre-se, para obter essas estimativas, temos que maximizar a Equation¬†1, ou equivalentemente a Equation¬†2.\nNo gr√°fico acima, √© poss√≠vel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. N√£o iremos fazer testes de adequa√ß√£o!\nDe fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de $X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)$. Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de $X$. Segue logo abaixo:\n$$f_X(x) = (\\alpha/\\beta)(x/\\beta)^{\\alpha - 1}\\exp[{-(x/\\beta)^\\alpha}],$$ com $x$, $\\alpha$ e $\\beta \u0026gt; 0$.\nImplementa√ß√µes  #   Agora que j√° conhecemos $f_X$ e $\\ell(\\cdot)$ (\u0026ldquo;fun√ß√£o objetivo\u0026rdquo;), poderemos colocar as \u0026ldquo;m√£os na massa\u0026rdquo; no teclado.\n E os dados?\nOs dados ser√£o gerados aleatoriamente, em cada uma das linguagens (R, Julia e Python). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° das implementa√ß√µes dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a $\\alpha = 2.5$ e $\\beta = 1.5$, nas tr√™s linguagens. Por isso, n√£o irie comparar, inicialmente, os resultados das estimativas obtidas. Ao final da postagem, farei uma sucinta compara√ß√£o.\nIrei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles. Nada em excesso!\n Antes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?\n  Numpy: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£o de n√∫meros pseudo-aleat√≥rios, entre outras coisas. Consulte mais detalhes em https://numpy.org/doc/stable;\n  Scipy: trata-se de outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos e algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integra√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em https://scipy.org.\n  Iremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o Weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html.\nNo link voc√™ ver√° que o que √© implementado pelo m√©todo random.weibull √© gerar observa√ß√µes de $X = [-\\log(U)]^{1/\\alpha} \\sim Weibull(\\alpha)$, com $U$ sendo um v.a. uniforme no intervalo (0,1] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o $Weibull(\\alpha, \\beta)$, teremos que multiplicar o resultado de random.weibull pelo valor de $\\beta$. Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da $Weibull(\\alpha, \\beta)$.\n   Veja como seria em R, Julia e Python:\nR  #    Code set.seed(0) rweibull(n = 10L, shape = 2.5, scale = 1.5)   [1] 0.6181884 1.6792787 1.4930932 1.1870595 0.5881789 1.8107341 0.6138897 [8] 0.4766274 1.0544363 1.1027820  Julia  #    Code using Distributions using Random Random.seed!(0); rand(Weibull(2.5,1.5), 10)  10-element Vector{Float64}: 1.3361401397866541 1.0507258710483653 0.8178793280596004 0.6700247875189858 0.6429703222489156 0.6315072585920245 2.313206178975804 2.048795844959291 1.1773995533033512 1.6047832712314394  Python  #    Code import numpy as np def random_weibull(n, alpha, beta): return beta * np.random.weibull(alpha, n) np.random.seed(0) random_weibull(n = 10, alpha = 2.5, beta = 1.5)  array([1.36908085, 1.64315124, 1.4528271 , 1.36309319, 1.18186313, 1.52263868, 1.20258335, 2.06494277, 2.42259084, 1.12172534])   Isso √© um pouco estranho, mas tudo bem, sabemos programar!\nTer que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o random_weibull, em que foi preciso considerar beta * np.random.weibull(alpha, n) para se ter observa√ß√µes Weibull com valores de $\\beta$ diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.\nAfinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que $\\beta = 1$, como ocorre em random.weibull ele poderia especificar isso como argumento passados √† fun√ß√£o, certo?\nComportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de prop√≥sito geral, assim como Python √© em sua ess√™ncia.\nNote que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo random.weibull e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as novidades de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.\n Agora, sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.\n   Nas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamento de c√≥digo:\n  Note que utilizo o conceito de fun√ß√µes com argumentos vari√°veis, tamb√©m chamadas de fun√ß√µes varargs. Perceba que a fun√ß√£o log_likelihood n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de log_likelihood nos tr√™s c√≥digos (R, Python e Julia). Precisamos de fun√ß√µes com operadores varargs, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos varargs √© uma t√©cnica muito poderosa. Utilizei esses conceitos em Python e Julia, devido a natureza das fun√ß√µes de otimiza√ß√µes das duas linguagens, em que em seus designers permitem que os argumentos a serem otimizados da fun√ß√£o objetivo seja de n√∫mero vari√°vel. As fun√ß√µes varargs de R s√£o definidas pelo operador dot-dot-dot (...). O designer da fun√ß√£o optim() de R incluem o par√¢metro dot-dot-dot, para argumentos extras que eventualmente possam existir na fun√ß√£o objetivo. ‚ö°\n  Note que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para $\\alpha$ e $\\beta$ . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se ad√©que a diversas outras situa√ß√µes!\n  Outro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecidas como fun√ß√µes lambda.\n  Foi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo BFGS. Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de Broyden\u0026ndash;Fletcher\u0026ndash;Goldfarb\u0026ndash;Shanno - BFGS pertencem, nos materiais que disponibilizo aos meus alunos na disciplina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes a respeito dos m√©todos de Newtone quasi-Newton, clique aqui.\n   Minimizar ou maximizar?\nAlguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, $-f$ equivale a maximizar $f$, em que $f$ √© uma dada fun√ß√£o objetivo.\n R  #    Code # Quantidade de observa√ß√µes n \u0026lt;- 250L # Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o # da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes. alpha \u0026lt;- 2.5 beta \u0026lt;- 1.5 # Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios. # Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir # os mesmos dados. set.seed(0) # Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para # modelar. dados \u0026lt;- rweibull(n = n, shape = alpha, scale = beta) pdf_weibull \u0026lt;- function(x, par){ alpha \u0026lt;- par[1] beta \u0026lt;- par[2] alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha) } # Checando se a densidade de pdf_weibull integra em 1 area = integrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5)) # Em R, o operador dot-dot-dot (...) √© utilizado para definir # quantidade vari√°dica de argumentos. Assim, log_likelihood √© # uma fun√ß√£o vararg. log_likelihood \u0026lt;- function(x, pdf, par) -sum(log(pdf(x, par))) result \u0026lt;- optim( fn = log_likelihood, par = c(0.5, 0.5), method = \u0026#34;BFGS\u0026#34;, x = dados, pdf = pdf_weibull ) # Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa cat(\u0026#34;Valores estimados de alpha e beta\\n\u0026#34;)  Valores estimados de alpha e beta   Code cat(\u0026#34;--\u0026gt; alpha: \u0026#34;, result$par[1], \u0026#34;\\n\u0026#34;)  --\u0026gt; alpha: 2.834629   Code cat(\u0026#34;--\u0026gt; beta: \u0026#34;, result$par[2], \u0026#34;\\n\u0026#34;)  --\u0026gt; beta: 1.456748  Julia  #    Code using Distributions using Random using Optim using QuadGK # Quantidade de observa√ß√µes n = 250; # Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., # da fdp da v.a. cujos os dados s√£o observa√ß√µes. Œ± = 2.5; Œ≤ = 1.5; # Fixando um valor de semente Random.seed!(0); # Gerando um array de dados com distribui√ß√£o Weibull(Œ±, Œ≤) dados = rand(Weibull(Œ±,Œ≤), n); # Fun√ß√£o densidade de probaiblidade de uma v.a. # X ‚àº Weibull(Œ±, Œ≤) function pdf_weibull(x, par = (Œ±, Œ≤)) Œ±, Œ≤ = par.Œ±, par.Œ≤ @. Œ±/Œ≤ * (x/Œ≤)^(Œ±-1) * exp(-(x/Œ≤)^Œ±) end; # Checando se a integral no suporte de pdf_weibull integra em 1 area, error = quadgk(x -\u0026gt; pdf_weibull(x, (Œ± = Œ±, Œ≤ = Œ≤)), 0, Inf); # Escrevendo a fun√ß√£o log_likelihood que em julia denotarei por # ‚Ñì, tendo em vista que podemos fazer uso de caracteres UTF-8 # nessa linguagem. function ‚Ñì(x, pdf, par...) -sum(log.(pdf(x, par...))) end; # Encontrando as estimativas de m√°xima verossimilhan√ßa usando a # biblioteca Optim emv = optimize( x -\u0026gt; ‚Ñì(dados, pdf_weibull, (Œ± = x[1], Œ≤ = x[2])), [0.5, 0.5], LBFGS() ); emv_Œ±, emv_Œ≤ = emv.minimizer; # Imprimindo o resultado print(\u0026#34;Valores estimados para Œ± e Œ≤\\n\u0026#34;)  Valores estimados para Œ± e Œ≤   Code print(\u0026#34;--\u0026gt; Œ±: \u0026#34;, emv_Œ±, \u0026#34;\\n\u0026#34;)  --\u0026gt; Œ±: 2.4472234393032837   Code print(\u0026#34;--\u0026gt; Œ≤: \u0026#34;, emv_Œ≤, \u0026#34;\\n\u0026#34;)  --\u0026gt; Œ≤: 1.4480344422787308  Python  #    Code import numpy as np import scipy.stats as stat import scipy.integrate as inte import scipy.optimize as opt # Valores da distribui√ß√£o verdadeira alpha = 2.5 beta = 1.5 # N√∫mero de observa√ß√µes que ir√£o compor nossos dados n = 250 # Implementando a fun√ß√£o random_weibull, em que os par√¢metros # que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais # sentido ser assim, n√£o? def random_weibull(n, alpha, beta): return beta * np.random.weibull(alpha,n) # Escrevendo a fun√ß√£p densidade de probabilidade da Weibull # na reparametriza√ß√£o correta. def pdf_weibull(x, param): alpha = param[0] beta = param[1] return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha) # Testando se a densidade integra em 1 √°rea = round(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]), 0, np.inf, args = (1,1))[0],2) # Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo # (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o # ir√° receber como argumento uma fun√ß√£o densidade de probabilidade. # N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de # log-verossimilhan√ßa! # A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1 # devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar # uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f. # Lembre-se disso! def log_likelihood(x, pdf, *args): return -np.sum(np.log(pdf(np.array(x), *args))) # Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa # √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera # que gerou os dados que desejamos ajustar. # Precisamos fixar uma semente, uma vez que queremos os mesmos dados # toda vez que rodamos esse c√≥digo. np.random.seed(0) dados = random_weibull(n = n, alpha = alpha, beta = beta) # Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando # a fun√ß√£o log_likelihood. alpha, beta = opt.minimize( fun = lambda *args: log_likelihood(dados, pdf_weibull, *args), x0=[0.5, 0.5] ).x # Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa print(\u0026#34;Valores estimados de alpha e beta\\n\u0026#34;)  Valores estimados de alpha e beta   Code print(\u0026#34;--\u0026gt; alpha: \u0026#34;, alpha, \u0026#34;\\n\u0026#34;)  --\u0026gt; alpha: 2.541634351446927   Code print(\u0026#34;--\u0026gt; beta: \u0026#34;, beta, \u0026#34;\\n\u0026#34;)  --\u0026gt; beta: 1.4804305843764871  Visualiza√ß√£o gr√°fica  #   Vamos agorar comparar graficamente as estimativas obtidas pelos m√©todos de otimiza√ß√£o (BFGS) considerando as bibliotecas fornecidas nas tr√™s linguages. Para uma melhor visualiza√ß√£o, utilizaremos o gr√°fico de curvas de n√≠veis. Perceba que $\\mathcal{L}$ possui como vari√°veis os par√¢metros $\\alpha$ e $\\beta$, sendo $x$ o conjunto de dados que √© fixo. Assim, consiguiremos visualizar em 2D o comportamento de $\\mathcal{L}$.\nPara que possamos visualizar graficamente e comparar as estimativas obtidas nas tr√™s linguagens, ser√° preciso que o conjunto de dados seja id√™ntico nos processos de otimiza√ß√£o realizados em R, Julia e Python.\nPara acessar o conjunto de dados, clique aqui. O conjunto de dados possui $n = 500$ observa√ß√µes. Ainda continuarei considerando $\\alpha = 2.5$ e $\\beta = 1.5$.\n Quer ver o c√≥digo do gr√°fico? Clique aqui! library(ggplot2) library(tidyr) library(dplyr) library(latex2exp) set.seed(0) dados \u0026lt;- rweibull(n = 500L, shape = 2.5, scale = 1.5) pdf_weibull \u0026lt;- function(x, alpha, beta) alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha) # log-verossimilhan√ßa log_likelihood \u0026lt;- function(x, alpha, beta) prod(pdf_weibull(x, alpha, beta)) vec_log_likelihood \u0026lt;- Vectorize( FUN = log_likelihood, vectorize.args = c(\u0026#34;alpha\u0026#34;, \u0026#34;beta\u0026#34;) ) alpha \u0026lt;- seq(2.4, 2.87, length.out = length(dados)) beta \u0026lt;- seq(1.42, 1.56, length.out = length(dados)) df_contour \u0026lt;- expand_grid(alpha, beta) df_contour \u0026lt;- df_contour |\u0026gt; mutate(z = vec_log_likelihood(x = dados, alpha, beta)) df_contour |\u0026gt; ggplot() + geom_contour_filled(aes(x = alpha, y = beta, z = z), show.legend = FALSE) + ggtitle( label = \u0026#34;Curvas de n√≠veis da fu√ß√£o de Verossimilhan√ßa\u0026#34;, subtitle = \u0026#34;Comparativo R, Python e Julia\u0026#34; ) + xlab(TeX(r\u0026#39;(\\alpha)\u0026#39;)) + ylab(TeX(r\u0026#39;(\\beta)\u0026#39;)) + labs(fill = \u0026#34;N√≠veis\u0026#34;) + theme( plot.title = element_text(face = \u0026#34;bold\u0026#34;), legend.title = element_text(face = \u0026#34;bold\u0026#34;) ) + # R geom_point( x = 2.64276, y = 1.488277 , color = \u0026#34;blue\u0026#34;, size = 3 ) + # Python geom_point( x = 2.642850353244295, y = 1.488271661049081, color = \u0026#34;green\u0026#34;, size = 2.5 ) + # Julia geom_point( x = 2.642850346755736, y = 1.4882716649411558 , color = \u0026#34;red\u0026#34;, size = 2 )  Perceba que foi obtido √≥timas estimativas pelo m√©todo BFGS, nas tr√™s linguagens. Note que tive que colocar um ponto maior que o outro, para que eles n√£o ficassem totalmente sobrepostos, e assim pud√©ssemos visualizar.\nConclus√µes  #   Espero que esse post tenha conseguido exemplificar como poderemos implementar a solu√ß√£o de um problema de estima√ß√£o por m√°xima verossimilhan√ßa, utilizando as linguagens de programa√ß√£o R, Python e Julia. Abordamos conceitos interessantes como fun√ß√µes varargs e fun√ß√µes an√¥nimas, al√©m de algumas estruturas de dados e bibliotecas.\nNos primeiros c√≥digos, foram observados √≥timas estimativas, muito embora elas n√£o s√£o compar√°veis, tendo em vista que os conjuntos de dados gerados aleatoriamente foram distintos, por conta da natureza de implementa√ß√£o das fun√ß√µes para gera√ß√£o de n√∫meros pseudo-aleat√≥rios, dispon√≠veis em cada uma das linguagens comparadas. Foi escolhido fazer dessa forma, para que fosse poss√≠vel abordar o problema de gera√ß√£o de n√∫meros pseudo-aleat√≥rios em cada linguagem.\nNo fim, foi realizado uma compara√ß√£o mais justa, onde o mesmo conjunto de dados foi considerado para a produ√ß√£o das curvas de n√≠veis, √∫teis para visualizar em uma imagem 2D a qualidade das estimativas. No gr√°fico de curvas de n√≠veis, √© poss√≠vel observar a sobreposi√ß√£o das estimativas obtidas utilizando R, Python e Julia.\nTamb√©m √© poss√≠vel perceber que otimizar uma fun√ß√£o objetivo √© f√°cil, independentemente da linguagem, sendo poss√≠vel conseguir c√≥digos concisos e eficientes.\nTodos os gr√°ficos dessa postagem foram constru√≠dos em R, atrav√©s da biblioteca Wickham (2016). Caso queira construir gr√°ficos em Python ou em Julia, considere as seguintes cito a biblioteca seaborn de Python e a biblioteca Makie de Julia. S√£o apenas sugest√µes, tendo em vista que h√° diversas outras alternativas interessantes.\nRefer√™ncias  #   Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B. Shah. 2017. \u0026ldquo;Julia: A Fresh Approach to Numerical Computing.\u0026rdquo; SIAM Review 59 (1): 65\u0026ndash;98. https://doi.org/10.1137/141000671.\nLi, Changcheng. 2019. \u0026ldquo;JuliaCall: An r Package for Seamless Integration Between r and Julia\u0026rdquo; 4: 1284. https://doi.org/10.21105/joss.01284.\nMarinho, Pedro Rafael Diniz, Gauss M. Cordeiro, Fernando Pe√±a Ram√≠rez, Morad Alizadeh, and Marcelo Bourguignon. 2018. \u0026ldquo;The Exponentiated Logarithmic Generated Family of Distributions and the Evaluation of the Confidence Intervals by Percentile Bootstrap.\u0026rdquo; Brazilian Journal of Probability and Statistics 32 (2). https://doi.org/10.1214/16-bjps343.\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\nRossum, Guido van. 2010. \u0026ldquo;Python 3000.\u0026rdquo; In, 313\u0026ndash;15. Apress. https://doi.org/10.1007/978-1-4302-2758-8_17.\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2022. \u0026ldquo;Reticulate: Interface to \u0026lsquo;Python\u0026rsquo;.\u0026rdquo; https://CRAN.R-project.org/package=reticulate.\nWickham, Hadley. 2016. \u0026ldquo;Ggplot2: Elegant Graphics for Data Analysis.\u0026rdquo; https://ggplot2.tidyverse.org.\n","date":"9 de Janeiro de 2022","permalink":"/posts/comparando_linguagens/","section":"Posts","summary":"Uma breve introdu√ß√£o  #   Ci√™ncia de dados √©, sem d√∫vidas, uma √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.","title":"Estima√ß√£o por m√°xima verossimilhan√ßa em R, Julia e Python"},{"content":"","date":"9 de Janeiro de 2022","permalink":"/categories/julia/","section":"Categories","summary":"","title":"Julia"},{"content":"","date":"9 de Janeiro de 2022","permalink":"/categories/python/","section":"Categories","summary":"","title":"Python"},{"content":" body { display: flex; align-items: center; } .content { text-align: center; } .image-container-members { display: flex; justify-content: space-around; align-items: left; margin-top: 20px; } .person { width: 200px; text-align: left; margin: 0 15px; } .person img { width: 100%; height: auto; margin-bottom: 10px; } .person p { margin-bottom: 5px; } .image-container { display: flex; align-items: center; margin-bottom: 20px; } .image-container img { max-width: 200px; height: auto; margin-left: 20px; } .text-container { flex: 1; display: flex; flex-direction: column; margin-right: 20px; } .text-container p { margin-bottom: 10px; } .members-heading { margin-top: 100px; }   Orientadores Prof. Dr. Eufr√°sio de Andrade Lima Neto  Gradua√ß√£o e mestrado em Estat√≠stica (UFPE) e doutorado em Ci√™ncias da computa√ß√£o (UFPE). √â Professor Associado III do Departamento de Estat√≠stica da UFPB com participa√ß√£o no Programa de P√≥s-Gradua√ß√£o Matem√°tica e Modelagem Computacional. Atua em: Modelagem Estat√≠stica, Aprendizagem de M√°quina, Regress√£o Robusta, Modelos Lineares Generalizados, An√°lise de Dados Simb√≥licos, Regress√£o Clusterwise e M√©todos baseado em Kernel.   Prof. Dr. Pedro Rafael Diniz Marinho  Gradua√ß√£o em Estat√≠stica (UFPB) e mestrado e doutorado em Estat√≠stica (UFPE). Foi bolsista da Funda√ß√£o de Amparo a Ci√™ncia e Tecnologia do Estado de Pernambuco. √â professor Adjunto do Departamento de Estat√≠stica da UFPB com participa√ß√£o no Programa de P√≥s-gradua√ß√£o em Modelagem Matem√°tica e Computacional. Atua em: Ci√™ncia de Dados, M√©todos Estat√≠sticos Computacionalmente Intensivos, Constru√ß√£o de Softwares pa ra a Estat√≠stica, Infer√™ncia Estat√≠stica Cl√°ssica e Probabilidade.   Prof. Dr. Marcelo Rodrigo Portela Ferreira  Gradua√ß√£o e mestrado em Estat√≠stica (UFPE), douturado em Ci√™ncia da Computa√ß√£o (UFPB) e p√≥s-douturado na RWTH Aachen University, Alemanha. √â professor Associado I do Departamento de stat√≠stica da UFPB com participa√ß√£o no Programa de P√≥s-Gradua√ß√£o em Modelos de Decis√£o e Sa√∫de. Atua em: Aprendizado de M√°quina, Ci√™ncia de Dados e Estat√≠stica Computacional, M√©todos n√£o-param√©tricos e an√°lise de dados simb√≥licos.   Prof. Dr. Luiz Medeiros de Araujo Lima Filho  Gradua√ß√£o em Estat√≠stica (UFPE), mestrado e doutorado em Biometria e Estat√≠stica Aplicada (UFRPE), p√≥s-doutorado com o professor pesquisador F√°bio Mariano Bayer (UFSM). √â Professor Associado I do Departamento de Estat√≠stica da UFPB com participa√ß√£o no Programa de P√≥s-Gradua√ß√£o em Modelos de Decis√£o e Sa√∫de. Atua em: controle estat√≠stico de processos e modelagem.   Equipe Paulo Ricardo S. Campana\nGraduando no curso de estat√≠stica - UFPB com prefer√™ncia pela √°rea de modelagem estat√≠stica e simula√ß√£o. Programador em R, C e Zig.\n Gabriel de Jesus Pereira\nGraduando no curso de estat√≠stica - UFPB com prefer√™ncia pela √°rea de modelagem estat√≠stica e probabilidade. Programador em R, Python e Elixir.\n Rafhael Aur√©lio da Silva\nGraduando no curso de estat√≠stica - UFPB com prefer√™ncia pela √°rea \"modelagem alguma coisa ou algum interesse al√©m da estat√≠stica\". Programador em R.\n   ","date":"1 de Janeiro de 0001","permalink":"/equipe/","section":"Enigma","summary":"body { display: flex; align-items: center; } .","title":""},{"content":" body { display: flex; align-items: center; } img { margin-left: 200px; margin-top: 40px; } .description { margin-bottom: 25px; }  Livros   Estude estat√≠stica, programa√ß√£o ou ci√™ncia de dados com as recomenda√ß√µes da Enigma.     An Introduction to Statistical Learning An Introduction to Statistica Learning (Gareth James, Daniela Witten, Trevor Hastie e Robert Tibshirani) apresenta algumas das mais importantes t√©cnicas de modelagem e predi√ß√£o. Este livro inclue t√≥picos como regress√£o linear, classifica√ß√£o, m√©todos de reamostragem, m√©todos baseados em arv√≥res. O livro √© disponibilizado gratuitamente em https://www.statlearning.com/ para a linguagem de programa√ß√£o Python ou R.\nTidy Modeling with R Este livro ensina a utilizar o pacote framework tidymodels, uma cole√ß√£o de pacotes na linguagem de programa√ß√£o R para modelagem e machine learning. Al√©m disto, este livro ter√° como objetivo tamb√©m ensinar como manter boas metodologias e pr√°ticas estat√≠stica durante a modelagem.\nR for Data Science R for Data Science (Hadley Wickham, 2017) ensina utilizar os principais pacotes para manipula√ß√£o, processamento, limpeza e visualiza√ß√£o de dados para a linguagem de programa√ß√£o R. Introduz tamb√©m t√≥picos mais avan√ßados como o pacote Arrow.\nVoc√™ ir√° aprender:\n Manipula√ß√£o, limpeza e processamento de dados com todo o ecossistema tidyverse. Visualiza√ß√£o de dados b√°sica e avan√ßada com o pacote ggplot.  Python for Data Analysis Python for Data Analysis (Wes Mckinney, 2012) est√° preocupado com toda a parte de manipula√ß√£o, processamento e limpeza de dados utilizando a lingaugem Python. Ainda, pode ser considerado como uma introdu√ß√£o √† computa√ß√£o cient√≠fica em Python, fazendo uso de bibliotecas que tem isso como objetivo.\nNeste livro voc√™ ir√° aprender:\n O b√°sico e avan√ßado da biblioteca Numpy para computa√ß√£o cient√≠fica em Python. Manipula√ß√£o, limpeza e processamento de dados com a biblioteca Pandas. Visualiza√ß√£o de dados utilizando as bibliotecas Seaborn e Matplotlib.  Beej's Guide to C Programming Este livro introduz C, uma das linguagens de programa√ß√£o mais utilizadas e conhecidas, al√©m de ter inspirado tantas outras. O livro est√° dispon√≠vel em https://beej.us/guide/bgc/html/split/\n","date":"1 de Janeiro de 0001","permalink":"/materiais/","section":"Enigma","summary":"body { display: flex; align-items: center; } img { margin-left: 200px; margin-top: 40px; } .","title":""},{"content":"","date":"1 de Janeiro de 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 de Janeiro de 0001","permalink":"/equipe/","section":"","summary":"","title":"Equipe"},{"content":"","date":"1 de Janeiro de 0001","permalink":"/materiais/","section":"","summary":"","title":"Materiais"},{"content":"","date":"1 de Janeiro de 0001","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"1 de Janeiro de 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 de Janeiro de 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]